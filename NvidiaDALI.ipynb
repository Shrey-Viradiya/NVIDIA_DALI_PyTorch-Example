{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NvidiaDALI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4V+ZcI4TAw9bybeYmiKsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e40cd6b78ab453b9cb7e3dc4ea688fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd274081ee4d4985a2be374de472e605",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f106b38f00a4a5382dad053c273131b",
              "IPY_MODEL_4d855cb62b3f409990c20d01c6e9d37d"
            ]
          }
        },
        "cd274081ee4d4985a2be374de472e605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f106b38f00a4a5382dad053c273131b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3e3d39e937c47f9af4ca899823c839d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62fcdc72d41d44cfb28bdd31e12ae0b6"
          }
        },
        "4d855cb62b3f409990c20d01c6e9d37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dab43978f9e34fd1b784df36e092ed29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:05&lt;00:00, 8.26MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2402665425b648779943f21bc7a37bb8"
          }
        },
        "a3e3d39e937c47f9af4ca899823c839d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62fcdc72d41d44cfb28bdd31e12ae0b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dab43978f9e34fd1b784df36e092ed29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2402665425b648779943f21bc7a37bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrey-Viradiya/NVIDIA_DALI_PyTorch-Example/blob/main/NvidiaDALI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuzzPJrv86Lv",
        "outputId": "dbb65db8-8fbf-4bfa-d359-426e459fe3b7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 27 12:25:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWbfNeC08HhW",
        "outputId": "4b629ec0-6688-4cf9-abca-236e14940515"
      },
      "source": [
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda100"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Collecting nvidia-dali-cuda100\n",
            "\u001b[?25l  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda100/nvidia_dali_cuda100-0.31.0-2055431-py3-none-manylinux2014_x86_64.whl (389.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 389.9MB 41kB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-dali-cuda100\n",
            "Successfully installed nvidia-dali-cuda100-0.31.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGUJ_jv-TRb"
      },
      "source": [
        "import os\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "import numpy as np\r\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator\r\n",
        "from nvidia.dali.pipeline import Pipeline\r\n",
        "import nvidia.dali.ops as ops\r\n",
        "import nvidia.dali.types as types\r\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator\r\n",
        "\r\n",
        "# import types\r\n",
        "import collections\r\n",
        "import pandas as pd\r\n",
        "from random import shuffle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kGWGgmAIBjR"
      },
      "source": [
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip \\\r\n",
        "  && unzip -qq flower_data.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORyTPIca_gsk"
      },
      "source": [
        "# !wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip \\\r\n",
        "#   && unzip -qq flower_data.zip \\\r\n",
        "#   && mkdir -p ./flower_data/flower_data_flat \\\r\n",
        "#   && find ./flower_data/train -mindepth 2 -type f -exec mv -t ./flower_data/flower_data_flat -i '{}' +"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "devNUodXBiG1"
      },
      "source": [
        "images_directory = '/content/flower_data/train'\r\n",
        "# read names of all image files\r\n",
        "# image_files = [f for f in listdir(images_directory) if isfile(join(images_directory, f))]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo9PJ10zIIZn"
      },
      "source": [
        "# # we create a data frame with the image names and dummy labels - label_1, label_2\r\n",
        "# labels = []\r\n",
        "# for i in range(len(image_files)):\r\n",
        "#     x = random.randint(0,4)\r\n",
        "#     labels.append(x)\r\n",
        "# data = pd.DataFrame(list(zip(image_files, labels)), columns=['image_filename', 'label'])\r\n",
        "\r\n",
        "# processed_data_file = 'flower_dummy_data.ssv'\r\n",
        "# data.to_csv(processed_data_file, index=False, header=False, sep=' ')\r\n",
        "\r\n",
        "# print(data.head())\r\n",
        "# #\timage_filename\tlabel_1\tlabel_2\r\n",
        "# # 0\timage_05973.jpg\t0\t    0\r\n",
        "# # 1\timage_00956.jpg\t1\t    1\r\n",
        "# # 2\timage_06047.jpg\t2\t    2\r\n",
        "# # 3\timage_07168.jpg\t3\t    3\r\n",
        "# # 4\timage_04466.jpg\t4\t    4"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01lxu8UyBFUy"
      },
      "source": [
        "# class ExternalInputIterator(object):\r\n",
        "#     def __init__(self, batch_size, data_file, images_directory):\r\n",
        "#         self.images_dir = images_directory\r\n",
        "#         self.batch_size = batch_size\r\n",
        "#         self.data_file = data_file\r\n",
        "#         with open(self.data_file, 'r') as f:\r\n",
        "#             self.files = [line.rstrip() for line in f if line is not '']\r\n",
        "\r\n",
        "#     def __iter__(self):\r\n",
        "#         self.i = 0\r\n",
        "#         self.n = len(self.files)\r\n",
        "#         shuffle(self.files)\r\n",
        "#         return self\r\n",
        "\r\n",
        "#     def __next__(self):\r\n",
        "\r\n",
        "#         if self.i >= self.n:\r\n",
        "#             self.__iter__()\r\n",
        "#             raise StopIteration\r\n",
        "\r\n",
        "#         batch = []\r\n",
        "#         labels = []\r\n",
        "#         for _ in range(self.batch_size):\r\n",
        "#             # *label reads multiple labels \r\n",
        "#             jpeg_filename, label = self.files[self.i % self.n].split(' ')\r\n",
        "#             f = open(images_directory + jpeg_filename, 'rb')\r\n",
        "#             batch.append(np.frombuffer(f.read(), dtype = np.uint8))\r\n",
        "#             labels.append(np.array(label, dtype = np.uint8))\r\n",
        "#             self.i += 1\r\n",
        "#         return (batch, labels)\r\n",
        "\r\n",
        "#     next = __next__"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z61ZlMWbBqrR"
      },
      "source": [
        "# eii = ExternalInputIterator(batch_size=16, \r\n",
        "#                             data_file=processed_data_file, \r\n",
        "#                             images_directory=images_directory)\r\n",
        "# iterator = iter(eii)\r\n",
        "\r\n",
        "# class ExternalSourcePipeline(Pipeline):\r\n",
        "#     def __init__(self, data_iterator, batch_size, num_threads, device_id):\r\n",
        "#         super(ExternalSourcePipeline, self).__init__(batch_size,\r\n",
        "#                                       num_threads,\r\n",
        "#                                       device_id,\r\n",
        "#                                       seed=12)\r\n",
        "#         self.data_iterator = data_iterator\r\n",
        "#         self.input = ops.ExternalSource()\r\n",
        "#         self.input_label = ops.ExternalSource()\r\n",
        "#         self.decode = ops.ImageDecoder(device = \"mixed\", output_type = types.RGB)\r\n",
        "#         # resizing is *must* because loaded images maybe of different sizes\r\n",
        "#         # and to create GPU tensors we need image arrays to be of same size\r\n",
        "#         self.res = ops.Resize(device=\"gpu\", resize_x=224, resize_y=224, interp_type=types.INTERP_TRIANGULAR)\r\n",
        "\r\n",
        "#     def define_graph(self):\r\n",
        "#         self.jpegs = self.input()\r\n",
        "#         self.labels = self.input_label()\r\n",
        "#         images = self.decode(self.jpegs)\r\n",
        "#         output = self.res(images)\r\n",
        "#         return (output, self.labels)\r\n",
        "\r\n",
        "#     def iter_setup(self):\r\n",
        "#         # the external data iterator is consumed here and fed as input to Pipeline\r\n",
        "#         images, labels = self.data_iterator.next()\r\n",
        "#         self.feed_input(self.jpegs, images)\r\n",
        "#         self.feed_input(self.labels, labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muP2xYosIcKV"
      },
      "source": [
        "class HybridPipeline(Pipeline):\r\n",
        "    def __init__(self, batch_size, num_threads, device_id, images_directory):\r\n",
        "        super(HybridPipeline, self).__init__(batch_size, num_threads, device_id, seed = 12)\r\n",
        "        self.input = ops.FileReader(file_root = images_directory, random_shuffle = True, initial_fill = 21)\r\n",
        "        self.decode = ops.ImageDecoder(device = \"mixed\", output_type = types.RGB)\r\n",
        "        self.rotate = ops.Rotate(device = \"gpu\")\r\n",
        "        self.rng = ops.random.Uniform(range = (-10.0, 10.0))\r\n",
        "        self.coin = ops.random.CoinFlip(probability = 0.5)\r\n",
        "        self.res = ops.Resize(device=\"gpu\", resize_x=224, resize_y=224, interp_type=types.INTERP_TRIANGULAR)\r\n",
        "        self.flip = ops.Flip(device = \"gpu\")\r\n",
        "\r\n",
        "    def define_graph(self):\r\n",
        "        jpegs, labels = self.input()\r\n",
        "        images = self.decode(jpegs)\r\n",
        "        angle = self.rng()\r\n",
        "        images = self.rotate(images, angle=angle)\r\n",
        "        images = self.flip(images, horizontal = self.coin(), vertical = self.coin())\r\n",
        "        images = self.res(images)\r\n",
        "        # images are on the GPU\r\n",
        "        return (images, labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbcXhnpRBsTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7c43d5-f6e3-40e3-fee0-5b1b6a883645"
      },
      "source": [
        "from nvidia.dali.plugin.pytorch import DALIClassificationIterator\r\n",
        "\r\n",
        "# pipe = ExternalSourcePipeline(data_iterator=iterator, batch_size=16, num_threads=2, device_id=0)\r\n",
        "# pipe.build()\r\n",
        "\r\n",
        "pipe = HybridPipeline(batch_size=16, num_threads=2, device_id=0, images_directory=images_directory)\r\n",
        "pipe.build()\r\n",
        "\r\n",
        "# first parameter is list of pipelines to run\r\n",
        "# second pipeline is output_map that maps consecutive outputs to corresponding names\r\n",
        "dali_iter = DALIClassificationIterator([pipe], size=409)\r\n",
        "\r\n",
        "pipe2 = HybridPipeline(batch_size=2, num_threads=2, device_id=0, images_directory=\"/content/flower_data/valid/\")\r\n",
        "pipe2.build()\r\n",
        "\r\n",
        "valid_iter = DALIClassificationIterator([pipe2], size=409)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/plugin/base_iterator.py:157: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.\n",
            "  _iterator_deprecation_warning()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW08uvtlB8bR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9e40cd6b78ab453b9cb7e3dc4ea688fe",
            "cd274081ee4d4985a2be374de472e605",
            "3f106b38f00a4a5382dad053c273131b",
            "4d855cb62b3f409990c20d01c6e9d37d",
            "a3e3d39e937c47f9af4ca899823c839d",
            "62fcdc72d41d44cfb28bdd31e12ae0b6",
            "dab43978f9e34fd1b784df36e092ed29",
            "2402665425b648779943f21bc7a37bb8"
          ]
        },
        "outputId": "3da475a9-6ca0-4e1e-d12b-f9d0b6048fc0"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda\") \r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "model = torchvision.models.resnet18(pretrained=True)\r\n",
        "for name, param in model.named_parameters():\r\n",
        "    param.requires_grad = True\r\n",
        "\r\n",
        "model.fc = torch.nn.Sequential(\r\n",
        "                torch.nn.Linear(model.fc.in_features, 500),\r\n",
        "                torch.nn.ReLU(),\r\n",
        "                torch.nn.Dropout(),\r\n",
        "                torch.nn.Linear(500, 102),\r\n",
        "            )\r\n",
        "model.to(device)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e40cd6b78ab453b9cb7e3dc4ea688fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZNYNE6QIoUn",
        "outputId": "594118d1-e151-4db3-e592-f3a8faa068db"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, it in enumerate(dali_iter):\r\n",
        "        batch_data = it[0]\r\n",
        "        inputs, labels = batch_data[\"data\"].to(device, dtype=torch.float32), batch_data[\"label\"].to(device, dtype=torch.int64).squeeze()\r\n",
        "        inputs = inputs.permute(0,3,1,2)\r\n",
        "        \r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = model(inputs)\r\n",
        "        \r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "\r\n",
        "    \r\n",
        "    dali_iter.reset()\r\n",
        "\r\n",
        "    test_correct = 0\r\n",
        "    test_total = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, it in enumerate(valid_iter):\r\n",
        "            batch_data = it[0]\r\n",
        "            inputs, labels = batch_data[\"data\"].to(device), batch_data[\"label\"].to(device)\r\n",
        "            inputs = inputs.permute(0,3,1,2).float()\r\n",
        "            labels = labels.long()\r\n",
        "\r\n",
        "            test_output = model(inputs)\r\n",
        "\r\n",
        "            _, test_predicted = torch.max(test_output.data, 1)\r\n",
        "\r\n",
        "            test_total += labels.size(0)\r\n",
        "            test_ccount = (test_predicted == labels).sum().item()\r\n",
        "            test_correct += test_ccount\r\n",
        "    valid_iter.reset()\r\n",
        "    print(f\"[{epoch}] :  Running Loss = {running_loss}, Correct = {test_correct}, total = {test_total}\")\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] :  Running Loss = 118.76936984062195, Correct = 18, total = 410\n",
            "[1] :  Running Loss = 108.94468307495117, Correct = 27, total = 408\n",
            "[2] :  Running Loss = 101.47938251495361, Correct = 30, total = 410\n",
            "[3] :  Running Loss = 100.3052875995636, Correct = 32, total = 408\n",
            "[4] :  Running Loss = 99.65138673782349, Correct = 24, total = 410\n",
            "[5] :  Running Loss = 96.86003232002258, Correct = 35, total = 408\n",
            "[6] :  Running Loss = 90.18352341651917, Correct = 28, total = 410\n",
            "[7] :  Running Loss = 89.69748067855835, Correct = 25, total = 408\n",
            "[8] :  Running Loss = 93.55246686935425, Correct = 35, total = 410\n",
            "[9] :  Running Loss = 84.10302639007568, Correct = 32, total = 408\n",
            "[10] :  Running Loss = 87.97801351547241, Correct = 33, total = 410\n",
            "[11] :  Running Loss = 83.00080513954163, Correct = 35, total = 408\n",
            "[12] :  Running Loss = 83.99591112136841, Correct = 29, total = 410\n",
            "[13] :  Running Loss = 78.60520029067993, Correct = 33, total = 408\n",
            "[14] :  Running Loss = 78.66701817512512, Correct = 41, total = 410\n",
            "[15] :  Running Loss = 74.94670629501343, Correct = 47, total = 408\n",
            "[16] :  Running Loss = 73.49090623855591, Correct = 43, total = 410\n",
            "[17] :  Running Loss = 74.21920442581177, Correct = 39, total = 408\n",
            "[18] :  Running Loss = 75.14312696456909, Correct = 37, total = 410\n",
            "[19] :  Running Loss = 67.62007880210876, Correct = 48, total = 408\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIVDaKszWz4_",
        "outputId": "54cbba7f-353f-48e0-8309-dd7105d63fc9"
      },
      "source": [
        "!find \"/content/flower_data/train/\" -type f -print | wc -l\r\n",
        "!find \"/content/flower_data/valid/\" -type f -print | wc -l"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6552\n",
            "818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_FbIfHRcg5h"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}